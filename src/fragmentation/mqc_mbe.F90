!! Many-Body Expansion (MBE) calculation module
module mqc_mbe
   !! Implements hierarchical many-body expansion for fragment-based quantum chemistry
   !! calculations with MPI parallelization and energy/gradient computation.
   use pic_types, only: int32, int64, dp
   use pic_timer, only: timer_type
   use pic_blas_interfaces, only: pic_gemm, pic_dot
   use pic_mpi_lib, only: comm_t, send, recv, iprobe, MPI_Status, MPI_ANY_SOURCE, MPI_ANY_TAG
   use pic_logger, only: logger => global_logger
   use pic_io, only: to_char

   use mqc_mpi_tags, only: TAG_WORKER_REQUEST, TAG_WORKER_FRAGMENT, TAG_WORKER_FINISH, &
                           TAG_WORKER_SCALAR_RESULT, TAG_WORKER_MATRIX_RESULT, &
                           TAG_NODE_REQUEST, TAG_NODE_FRAGMENT, TAG_NODE_FINISH, &
                           TAG_NODE_SCALAR_RESULT, TAG_NODE_MATRIX_RESULT
   use mqc_physical_fragment, only: system_geometry_t, physical_fragment_t, build_fragment_from_indices, to_angstrom
   use mqc_elements, only: element_number_to_symbol
   use mqc_frag_utils, only: next_combination, find_fragment_index

   ! Method API imports
#ifndef MQC_WITHOUT_TBLITE
   use mqc_method_xtb, only: xtb_method_t
#endif
   use mqc_result_types, only: calculation_result_t
   implicit none
   private

   ! Public interface
   public :: process_chemistry_fragment, global_coordinator, node_coordinator
   public :: node_worker, unfragmented_calculation

contains

   subroutine process_chemistry_fragment(fragment_idx, fragment_indices, fragment_size, matrix_size, &
                                         water_energy, C_flat, method, phys_frag)
      !! Process a single fragment for quantum chemistry calculation
      !!
      !! Performs energy and gradient calculation on a molecular fragment using
      !! specified quantum chemistry method (GFN-xTB variants).
      !! Verbosity is controlled by the global logger level.

      use pic_logger, only: verbose_level

      integer, intent(in) :: fragment_idx        !! Fragment index for identification
      integer, intent(in) :: fragment_size       !! Number of monomers in fragment
      integer, intent(in) :: fragment_indices(fragment_size)  !! Monomer indices comprising this fragment
      integer, intent(in) :: matrix_size         !! Size of gradient matrix (natoms*3)
      real(dp), intent(out) :: water_energy      !! Computed energy for this fragment
      real(dp), allocatable, intent(out) :: C_flat(:)  !! Flattened gradient array
      character(len=*), intent(in) :: method     !! QC method (gfn1, gfn2)
      type(physical_fragment_t), intent(in), optional :: phys_frag  !! Fragment geometry

      integer :: current_log_level  !! Current logger verbosity level
      logical :: is_verbose  !! Whether verbose output is enabled
#ifndef MQC_WITHOUT_TBLITE
      type(xtb_method_t) :: xtb_calc  !! XTB calculator instance
#endif
      type(calculation_result_t) :: result  !! Computation results

      ! Query logger to determine verbosity
      call logger%configuration(level=current_log_level)
      is_verbose = (current_log_level >= verbose_level)

      ! Print fragment geometry if provided and verbose mode is enabled
      if (present(phys_frag)) then
         if (is_verbose) then
            call print_fragment_xyz(fragment_idx, phys_frag)
         end if

#ifndef MQC_WITHOUT_TBLITE
         ! Setup XTB method
         xtb_calc%variant = method
         xtb_calc%verbose = is_verbose

         ! Run the calculation using the method API
         call xtb_calc%calc_energy(phys_frag, result)
         water_energy = result%energy

         ! Clean up result
         call result%destroy()
#else
         call logger%error("XTB method requested but tblite support not compiled in")
         call logger%error("Please rebuild with -DMQC_ENABLE_TBLITE=ON")
         error stop "tblite support not available"
#endif
      else
         water_energy = 0.0_dp
      end if      ! Return empty vector for C_flat
      allocate (C_flat(1))
      C_flat(1) = 0.0_dp
   end subroutine process_chemistry_fragment

   function get_body_level_name(body_level) result(level_name)
      !! Map body level (n-mer) to descriptive name
      !! Supports up to decamers (10-mers), then falls back to "N-mers" format
      integer, intent(in) :: body_level
      character(len=32) :: level_name

      select case (body_level)
      case (1)
         level_name = "monomers"
      case (2)
         level_name = "dimers"
      case (3)
         level_name = "trimers"
      case (4)
         level_name = "tetramers"
      case (5)
         level_name = "pentamers"
      case (6)
         level_name = "hexamers"
      case (7)
         level_name = "heptamers"
      case (8)
         level_name = "octamers"
      case (9)
         level_name = "nonamers"
      case (10)
         level_name = "decamers"
      case default
         ! For levels > 10, use generic format
         write (level_name, '(i0,a)') body_level, "-mers"
      end select
   end function get_body_level_name

   subroutine print_fragment_xyz(fragment_idx, phys_frag)
      !! Print fragment geometry in XYZ format
      integer, intent(in) :: fragment_idx
      type(physical_fragment_t), intent(in) :: phys_frag
      integer :: i
      character(len=2) :: symbol
      character(len=256) :: coord_line

      call logger%info("=========================================")
      call logger%info(" Fragment "//to_char(fragment_idx))
      call logger%info(" Number of atoms: "//to_char(phys_frag%n_atoms))
      call logger%info(" Coordinates in Angstroms:")
      call logger%info("-----------------------------------------")
      do i = 1, phys_frag%n_atoms
         symbol = element_number_to_symbol(phys_frag%element_numbers(i))
         ! Convert from Bohr back to Angstroms for printing
         write (coord_line, '(a2,3f15.8)') symbol, to_angstrom(phys_frag%coordinates(1:3, i))
         call logger%info(trim(coord_line))
      end do
      call logger%info("=========================================")

   end subroutine print_fragment_xyz

   subroutine compute_mbe_energy(polymers, fragment_count, max_level, energies, total_energy)
      !! Compute the many-body expansion (MBE) energy
      !! Total = sum(E(i)) + sum(deltaE(ij)) + sum(deltaE(ijk)) + ...
      !! General n-body correction:
      !! deltaE(i1,i2,...,in) = E(i1,i2,...,in) - sum of all lower-order terms
      !! Uses int64 for fragment_count to handle large fragment counts that overflow int32.
      !! Detailed breakdown is printed only if logger level is verbose or higher.
      use pic_logger, only: verbose_level
      integer(int64), intent(in) :: fragment_count
      integer, intent(in) :: polymers(:, :), max_level
      real(dp), intent(in) :: energies(:)
      real(dp), intent(out) :: total_energy

      integer(int64) :: i
      integer :: fragment_size, body_level, current_log_level
      real(dp), allocatable :: sum_by_level(:), delta_energies(:)
      real(dp) :: delta_E
      logical :: do_detailed_print

      ! Query logger to decide if we should print detailed breakdown
      call logger%configuration(level=current_log_level)
      do_detailed_print = (current_log_level >= verbose_level)

      allocate (sum_by_level(max_level))
      allocate (delta_energies(fragment_count))
      sum_by_level = 0.0_dp
      delta_energies = 0.0_dp

      ! Sum over all fragments by their size
      do i = 1_int64, fragment_count
         fragment_size = count(polymers(i, :) > 0)

         if (fragment_size == 1) then
            ! 1-body terms: just the monomer energies
            delta_E = energies(i)
            delta_energies(i) = delta_E
            sum_by_level(1) = sum_by_level(1) + delta_E
         else if (fragment_size >= 2 .and. fragment_size <= max_level) then
            ! n-body corrections for n >= 2
            delta_E = compute_delta_nbody(polymers(i, 1:fragment_size), polymers, energies, &
                                          fragment_count, fragment_size)
            delta_energies(i) = delta_E
            sum_by_level(fragment_size) = sum_by_level(fragment_size) + delta_E
         end if
      end do

      total_energy = sum(sum_by_level)

      ! Print text summary to console
      call logger%info("MBE Energy breakdown:")
      do body_level = 1, max_level
         if (abs(sum_by_level(body_level)) > 1e-15_dp) then
            block
               character(len=256) :: energy_line
               write (energy_line, '(a,i0,a,f20.10)') "  ", body_level, "-body:  ", sum_by_level(body_level)
               call logger%info(trim(energy_line))
            end block
         end if
      end do
      block
         character(len=256) :: total_line
         write (total_line, '(a,f20.10)') "  Total:   ", total_energy
         call logger%info(trim(total_line))
      end block

      ! Print detailed breakdown if requested
      if (do_detailed_print) then
         call print_detailed_breakdown(polymers, fragment_count, max_level, energies, delta_energies)
      end if

      ! Always write JSON file for machine-readable output
      call print_detailed_breakdown_json(polymers, fragment_count, max_level, energies, delta_energies, &
                                         sum_by_level, total_energy)

      deallocate (sum_by_level, delta_energies)

   end subroutine compute_mbe_energy

   subroutine print_detailed_breakdown(polymers, fragment_count, max_level, energies, delta_energies)
      !! Print detailed energy breakdown for each fragment
      !! Shows full energy and deltaE correction for all monomers, dimers, trimers, etc.
      !! Uses int64 for fragment_count to handle large fragment counts that overflow int32.
      integer, intent(in) :: polymers(:, :), max_level
      integer(int64), intent(in) :: fragment_count
      real(dp), intent(in) :: energies(:), delta_energies(:)

      integer(int64) :: i
      integer :: fragment_size, j, body_level
      character(len=512) :: fragment_str, energy_line
      integer(int64) :: count_by_level

      call logger%verbose("")
      call logger%verbose("============================================")
      call logger%verbose("Detailed Energy Breakdown by Fragment")
      call logger%verbose("============================================")

      ! Warn if we have very high fragmentation levels
      if (max_level > 10) then
         call logger%warning("Fragment levels exceed decamers (10-mers). Using generic N-mers notation.")
      end if

      do body_level = 1, max_level
         count_by_level = 0_int64

         do i = 1_int64, fragment_count
            fragment_size = count(polymers(i, :) > 0)
            if (fragment_size == body_level) count_by_level = count_by_level + 1_int64
         end do

         if (count_by_level > 0_int64) then
            call logger%verbose("")
            block
               character(len=256) :: header
               character(len=32) :: level_name
               level_name = get_body_level_name(body_level)
               write (header, '(a,a,i0,a)') trim(level_name), " (", count_by_level, " fragments):"
               ! Capitalize first letter
               if (len_trim(level_name) > 0) then
                  if (level_name(1:1) >= 'a' .and. level_name(1:1) <= 'z') then
                     header(1:1) = achar(iachar(header(1:1)) - 32)
                  end if
               end if
               call logger%verbose(trim(header))
            end block
            call logger%verbose("--------------------------------------------")

            do i = 1_int64, fragment_count
               fragment_size = count(polymers(i, :) > 0)

               if (fragment_size == body_level) then
                  fragment_str = "["
                  do j = 1, fragment_size
                     if (j > 1) then
                        write (fragment_str, '(a,a,i0)') trim(fragment_str), ",", polymers(i, j)
                     else
                        write (fragment_str, '(a,i0)') trim(fragment_str), polymers(i, j)
                     end if
                  end do
                  write (fragment_str, '(a,a)') trim(fragment_str), "]"

                  if (body_level == 1) then
                     write (energy_line, '(a,a,f20.10)') &
                        "  Fragment ", trim(adjustl(fragment_str)), energies(i)
                  else
                     write (energy_line, '(a,a,f20.10,a,f20.10)') &
                        "  Fragment ", trim(adjustl(fragment_str)), energies(i), &
                        "   deltaE: ", delta_energies(i)
                  end if
                  call logger%verbose(trim(energy_line))
               end if
            end do
         end if
      end do

      call logger%verbose("")
      call logger%verbose("============================================")

   end subroutine print_detailed_breakdown

   subroutine print_detailed_breakdown_json(polymers, fragment_count, max_level, energies, delta_energies, sum_by_level, total_energy)
      !! Write detailed energy breakdown to results.json file
      !! Outputs structured JSON with all fragment energies and deltaE corrections
      !! Uses int64 for fragment_count to handle large fragment counts that overflow int32.
      integer, intent(in) :: polymers(:, :), max_level
      integer(int64), intent(in) :: fragment_count
      real(dp), intent(in) :: energies(:), delta_energies(:)
      real(dp), intent(in) :: sum_by_level(:), total_energy

      integer(int64) :: i
      integer :: fragment_size, j, body_level, unit, io_stat
      character(len=512) :: json_line
      integer(int64) :: count_by_level
      logical :: first_level, first_fragment
      character(len=32) :: level_name

      open (newunit=unit, file='results.json', status='replace', action='write', iostat=io_stat)
      if (io_stat /= 0) then
         call logger%error("Failed to open results.json for writing")
         return
      end if

      call logger%info("Writing JSON output to results.json")

      ! Warn if we have very high fragmentation levels
      if (max_level > 10) then
         call logger%warning("Fragment levels exceed decamers (10-mers). JSON will use generic N-mers notation.")
      end if

      write (unit, '(a)') "{"
      write (unit, '(a)') '  "mbe_breakdown": {'

      write (json_line, '(a,f20.10,a)') '    "total_energy": ', total_energy, ','
      write (unit, '(a)') trim(json_line)

      write (unit, '(a)') '    "levels": ['

      first_level = .true.
      do body_level = 1, max_level
         count_by_level = 0_int64

         do i = 1_int64, fragment_count
            fragment_size = count(polymers(i, :) > 0)
            if (fragment_size == body_level) count_by_level = count_by_level + 1_int64
         end do

         if (count_by_level > 0_int64) then
            if (.not. first_level) then
               write (unit, '(a)') '      },'
            end if
            first_level = .false.

            write (unit, '(a)') '      {'

            level_name = get_body_level_name(body_level)

            write (json_line, '(a,i0,a)') '        "body_level": ', body_level, ','
            write (unit, '(a)') trim(json_line)
            write (json_line, '(a,a,a)') '        "name": "', trim(level_name), '",'
            write (unit, '(a)') trim(json_line)
            write (json_line, '(a,i0,a)') '        "count": ', count_by_level, ','
            write (unit, '(a)') trim(json_line)
            write (json_line, '(a,f20.10,a)') '        "total_energy": ', sum_by_level(body_level), ','
            write (unit, '(a)') trim(json_line)
            write (unit, '(a)') '        "fragments": ['

            first_fragment = .true.
            do i = 1_int64, fragment_count
               fragment_size = count(polymers(i, :) > 0)

               if (fragment_size == body_level) then
                  if (.not. first_fragment) then
                     write (unit, '(a)') '          },'
                  end if
                  first_fragment = .false.

                  write (unit, '(a)') '          {'

                  json_line = '            "indices": ['
                  do j = 1, fragment_size
                     if (j > 1) then
                        write (json_line, '(a,a,i0)') trim(json_line), ', ', polymers(i, j)
                     else
                        write (json_line, '(a,i0)') trim(json_line), polymers(i, j)
                     end if
                  end do
                  write (json_line, '(a,a)') trim(json_line), '],'
                  write (unit, '(a)') trim(json_line)

                  write (json_line, '(a,f20.10)') '            "energy": ', energies(i)
                  if (body_level > 1) then
                     write (json_line, '(a,a)') trim(json_line), ','
                     write (unit, '(a)') trim(json_line)
                     write (json_line, '(a,f20.10)') '            "delta_energy": ', delta_energies(i)
                  end if
                  write (unit, '(a)') trim(json_line)
               end if
            end do

            if (.not. first_fragment) then
               write (unit, '(a)') '          }'
            end if
            write (unit, '(a)') '        ]'
         end if
      end do

      if (.not. first_level) then
         write (unit, '(a)') '      }'
      end if

      write (unit, '(a)') '    ]'
      write (unit, '(a)') '  }'
      write (unit, '(a)') '}'

      close (unit)
      call logger%info("JSON output written successfully to results.json")

   end subroutine print_detailed_breakdown_json

   recursive function compute_delta_nbody(fragment, polymers, energies, fragment_count, n) result(delta_E)
      !! Compute general n-body correction using recursion
      !! deltaE(i1,i2,...,in) = E(i1,i2,...,in)
      !!                        - sum over all proper subsets of deltaE or E
      !! For n=2: deltaE(ij) = E(ij) - E(i) - E(j)
      !! For n=3: deltaE(ijk) = E(ijk) - deltaE(ij) - deltaE(ik) - deltaE(jk) - E(i) - E(j) - E(k)
      !! For n>=4: same pattern recursively
      !! Uses int64 for fragment_count to handle large fragment counts that overflow int32.
      integer, intent(in) :: fragment(:), polymers(:, :), n
      integer(int64), intent(in) :: fragment_count
      real(dp), intent(in) :: energies(:)
      real(dp) :: delta_E

      integer(int64) :: idx_n
      integer :: subset_size, i, j, num_subsets
      integer, allocatable :: subset(:), indices(:)
      real(dp) :: E_n, subset_contribution

      ! Find the energy of this n-mer
      idx_n = find_fragment_index(fragment, polymers, fragment_count, n)
      E_n = energies(idx_n)

      ! Start with the full n-mer energy
      delta_E = E_n

      ! Subtract contributions from all proper subsets (size 1 to n-1)
      do subset_size = 1, n - 1
         ! Generate all subsets of this size and subtract their contributions
         call generate_and_subtract_subsets(fragment, subset_size, n, polymers, energies, &
                                            fragment_count, delta_E)
      end do

   end function compute_delta_nbody

   subroutine generate_and_subtract_subsets(fragment, subset_size, n, polymers, energies, &
                                            fragment_count, delta_E)
      !! Generate all subsets of given size and subtract their contribution
      !! Uses int64 for fragment_count to handle large fragment counts that overflow int32.
      integer, intent(in) :: fragment(:), subset_size, n, polymers(:, :)
      integer(int64), intent(in) :: fragment_count
      real(dp), intent(in) :: energies(:)
      real(dp), intent(inout) :: delta_E

      integer, allocatable :: indices(:), subset(:)
      integer :: i

      allocate (indices(subset_size))
      allocate (subset(subset_size))

      ! Initialize indices for first combination
      do i = 1, subset_size
         indices(i) = i
      end do

      ! Loop through all combinations
      do
         ! Build the current subset
         do i = 1, subset_size
            subset(i) = fragment(indices(i))
         end do

         ! Subtract this subset's contribution
         if (subset_size == 1) then
            ! 1-body: just subtract the monomer energy
            delta_E = delta_E - energies(find_fragment_index(subset, polymers, fragment_count, 1))
         else
            ! n-body: recursively compute and subtract deltaE for this subset
            delta_E = delta_E - compute_delta_nbody(subset, polymers, energies, fragment_count, subset_size)
         end if

         ! Get next combination
         if (.not. next_combination(indices, subset_size, n)) exit
      end do

      deallocate (indices, subset)

   end subroutine generate_and_subtract_subsets

   subroutine global_coordinator(world_comm, node_comm, total_fragments, polymers, max_level, &
                                 node_leader_ranks, num_nodes, matrix_size)
      !! Global coordinator for distributing fragments to node coordinators
      !! will act as a node coordinator for a single node calculation
      !! Uses int64 for total_fragments to handle large fragment counts that overflow int32.
      type(comm_t), intent(in) :: world_comm, node_comm
      integer(int64), intent(in) :: total_fragments
      integer, intent(in) :: max_level, num_nodes, matrix_size
      integer, intent(in) :: polymers(:, :), node_leader_ranks(:)

      integer(int64) :: current_fragment, results_received
      integer :: finished_nodes
      integer :: request_source, dummy_msg, fragment_idx
      type(MPI_Status) :: status, local_status
      logical :: handling_local_workers
      logical :: has_pending

      ! For local workers
      integer :: local_finished_workers, fragment_size, local_dummy
      integer, allocatable :: fragment_indices(:)

      ! Storage for results
      real(dp), allocatable :: scalar_results(:)
      real(dp), allocatable :: matrix_results(:, :)
      real(dp), allocatable :: temp_matrix(:)
      integer :: max_matrix_size
      integer :: worker_fragment_map(node_comm%size())
      integer :: worker_source

      current_fragment = total_fragments
      finished_nodes = 0
      local_finished_workers = 0
      handling_local_workers = (node_comm%size() > 1)
      results_received = 0_int64

      ! Allocate storage for results
      allocate (scalar_results(total_fragments))
      max_matrix_size = (max_level*matrix_size)**2
      allocate (matrix_results(max_matrix_size, total_fragments))
      scalar_results = 0.0_dp
      matrix_results = 0.0_dp
      worker_fragment_map = 0

      call logger%verbose("Global coordinator starting with "//to_char(total_fragments)// &
                          " fragments for "//to_char(num_nodes)//" nodes")

      do while (finished_nodes < num_nodes)

         ! PRIORITY 1: Check for incoming results from local workers
         ! This MUST be checked before sending new work to avoid race conditions
         if (handling_local_workers) then
            ! Keep checking for results until there are none pending
            do
               call iprobe(node_comm, MPI_ANY_SOURCE, TAG_WORKER_SCALAR_RESULT, has_pending, local_status)
               if (.not. has_pending) exit

               worker_source = local_status%MPI_SOURCE

               ! Safety check: worker should have a fragment assigned
               if (worker_fragment_map(worker_source) == 0) then
                  call logger%error("Received result from worker "//to_char(worker_source)// &
                                    " but no fragment was assigned!")
                  error stop "Invalid worker_fragment_map state"
               end if

               ! Receive scalar result and store it using the fragment index for this worker
         call recv(node_comm, scalar_results(worker_fragment_map(worker_source)), worker_source, TAG_WORKER_SCALAR_RESULT)
               ! Receive matrix result into temporary array, then copy to storage
               call recv(node_comm, temp_matrix, worker_source, TAG_WORKER_MATRIX_RESULT, local_status)
               ! Copy only the received size, pad rest with zeros
               matrix_results(1:size(temp_matrix), worker_fragment_map(worker_source)) = temp_matrix
               if (size(temp_matrix) < max_matrix_size) then
                  matrix_results(size(temp_matrix) + 1:max_matrix_size, worker_fragment_map(worker_source)) = 0.0_dp
               end if
               ! Clear the mapping since we've received the result
               worker_fragment_map(worker_source) = 0
               if (allocated(temp_matrix)) deallocate (temp_matrix)
               results_received = results_received + 1
            end do
         end if

         ! PRIORITY 1b: Check for incoming results from remote node coordinators
         do
            call iprobe(world_comm, MPI_ANY_SOURCE, TAG_NODE_SCALAR_RESULT, has_pending, status)
            if (.not. has_pending) exit

            ! Receive fragment index, scalar result, and matrix result from node coordinator
            ! TODO: serialize the data for better performance
            call recv(world_comm, fragment_idx, status%MPI_SOURCE, TAG_NODE_SCALAR_RESULT)
            call recv(world_comm, scalar_results(fragment_idx), status%MPI_SOURCE, TAG_NODE_SCALAR_RESULT)
            call recv(world_comm, temp_matrix, status%MPI_SOURCE, TAG_NODE_MATRIX_RESULT, status)

            ! Copy matrix result into storage
            matrix_results(1:size(temp_matrix), fragment_idx) = temp_matrix
            if (size(temp_matrix) < max_matrix_size) then
               matrix_results(size(temp_matrix) + 1:max_matrix_size, fragment_idx) = 0.0_dp
            end if
            if (allocated(temp_matrix)) deallocate (temp_matrix)
            results_received = results_received + 1
         end do

         ! PRIORITY 2: Remote node coordinator requests
         call iprobe(world_comm, MPI_ANY_SOURCE, TAG_NODE_REQUEST, has_pending, status)
         if (has_pending) then
            call recv(world_comm, dummy_msg, status%MPI_SOURCE, TAG_NODE_REQUEST)
            request_source = status%MPI_SOURCE

            if (current_fragment >= 1) then
               call send_fragment_to_node(world_comm, current_fragment, polymers, max_level, request_source)
               current_fragment = current_fragment - 1
            else
               call send(world_comm, -1, request_source, TAG_NODE_FINISH)
               finished_nodes = finished_nodes + 1
            end if
         end if

         ! PRIORITY 3: Local workers (shared memory) - send new work
         if (handling_local_workers .and. local_finished_workers < node_comm%size() - 1) then
            call iprobe(node_comm, MPI_ANY_SOURCE, TAG_WORKER_REQUEST, has_pending, local_status)
            if (has_pending) then
               ! Only process work request if this worker doesn't have pending results
               if (worker_fragment_map(local_status%MPI_SOURCE) == 0) then
                  call recv(node_comm, local_dummy, local_status%MPI_SOURCE, TAG_WORKER_REQUEST)

                  if (current_fragment >= 1) then
                     call send_fragment_to_worker(node_comm, current_fragment, polymers, max_level, &
                                                  local_status%MPI_SOURCE, matrix_size)
                     ! Track which fragment was sent to this worker
                     worker_fragment_map(local_status%MPI_SOURCE) = current_fragment
                     current_fragment = current_fragment - 1
                  else
                     call send(node_comm, -1, local_status%MPI_SOURCE, TAG_WORKER_FINISH)
                     local_finished_workers = local_finished_workers + 1
                  end if
               end if
               ! If worker still has pending results, skip the work request
               ! It will be processed on the next iteration after results are received
            end if
         end if

         ! Finalize local worker completion
         if (handling_local_workers .and. local_finished_workers >= node_comm%size() - 1 &
             .and. results_received >= total_fragments) then
            handling_local_workers = .false.
            if (num_nodes == 1) then
               finished_nodes = finished_nodes + 1
               call logger%debug("Manually incremented finished_nodes for self")
            else
               finished_nodes = finished_nodes + 1
               call logger%verbose("Global coordinator finished local workers")
            end if
         end if
      end do

      call logger%verbose("Global coordinator finished all fragments")
      block
         real(dp) :: mbe_total_energy

         ! Compute the many-body expansion energy
         call logger%info("")
         call logger%info("Computing Many-Body Expansion (MBE)...")
         call compute_mbe_energy(polymers, total_fragments, max_level, scalar_results, mbe_total_energy)

      end block

      ! Cleanup
      deallocate (scalar_results, matrix_results)
   end subroutine global_coordinator

   subroutine send_fragment_to_node(world_comm, fragment_idx, polymers, max_level, dest_rank)
      !! Send fragment data to remote node coordinator
      !! Uses int64 for fragment_idx to handle large fragment indices that overflow int32.
      type(comm_t), intent(in) :: world_comm
      integer(int64), intent(in) :: fragment_idx
      integer, intent(in) :: max_level, dest_rank
      integer, intent(in) :: polymers(:, :)
      integer :: fragment_size
      integer, allocatable :: fragment_indices(:)

      fragment_size = count(polymers(fragment_idx, :) > 0)
      allocate (fragment_indices(fragment_size))
      fragment_indices = polymers(fragment_idx, 1:fragment_size)

      ! TODO: serialize the data for better performance
      call send(world_comm, int(fragment_idx, kind=int32), dest_rank, TAG_NODE_FRAGMENT)
      call send(world_comm, fragment_size, dest_rank, TAG_NODE_FRAGMENT)
      call send(world_comm, fragment_indices, dest_rank, TAG_NODE_FRAGMENT)

      deallocate (fragment_indices)
   end subroutine send_fragment_to_node

   subroutine send_fragment_to_worker(node_comm, fragment_idx, polymers, max_level, dest_rank, matrix_size)
      !! Send fragment data to local worker
      !! Uses int64 for fragment_idx to handle large fragment indices that overflow int32.
      type(comm_t), intent(in) :: node_comm
      integer(int64), intent(in) :: fragment_idx
      integer, intent(in) :: max_level, dest_rank, matrix_size
      integer, intent(in) :: polymers(:, :)
      integer :: fragment_size
      integer, allocatable :: fragment_indices(:)

      fragment_size = count(polymers(fragment_idx, :) > 0)
      allocate (fragment_indices(fragment_size))
      fragment_indices = polymers(fragment_idx, 1:fragment_size)

      ! TODO: serialize the data for better performance
      call send(node_comm, int(fragment_idx, kind=int32), dest_rank, TAG_WORKER_FRAGMENT)
      call send(node_comm, fragment_size, dest_rank, TAG_WORKER_FRAGMENT)
      call send(node_comm, fragment_indices, dest_rank, TAG_WORKER_FRAGMENT)
      call send(node_comm, matrix_size, dest_rank, TAG_WORKER_FRAGMENT)

      deallocate (fragment_indices)
   end subroutine send_fragment_to_worker

   subroutine node_coordinator(world_comm, node_comm, max_level, matrix_size)
      !! Node coordinator for distributing fragments to local workers
      !! Handles work requests and result collection from local workers
      class(comm_t), intent(in) :: world_comm, node_comm
      integer(int32), intent(in) :: max_level, matrix_size

      integer(int32) :: fragment_idx, fragment_size, dummy_msg
      integer(int32) :: finished_workers
      integer(int32), allocatable :: fragment_indices(:)
      type(MPI_Status) :: status, global_status
      logical :: local_message_pending, more_fragments, has_result
      integer(int32) :: local_dummy

      ! For tracking worker-fragment mapping and collecting results
      integer(int32) :: worker_fragment_map(node_comm%size())
      integer(int32) :: worker_source
      real(dp) :: scalar_result
      real(dp), allocatable :: matrix_result(:)

      finished_workers = 0
      more_fragments = .true.
      dummy_msg = 0
      worker_fragment_map = 0

      do while (finished_workers < node_comm%size() - 1)

         ! PRIORITY 1: Check for incoming results from local workers
         call iprobe(node_comm, MPI_ANY_SOURCE, TAG_WORKER_SCALAR_RESULT, has_result, status)
         if (has_result) then
            worker_source = status%MPI_SOURCE

            ! Safety check: worker should have a fragment assigned
            if (worker_fragment_map(worker_source) == 0) then
               call logger%error("Node coordinator received result from worker "//to_char(worker_source)// &
                                 " but no fragment was assigned!")
               error stop "Invalid worker_fragment_map state in node coordinator"
            end if

            ! the send/recv operations need to be serialized
            ! Receive scalar and matrix results from worker
            call recv(node_comm, scalar_result, worker_source, TAG_WORKER_SCALAR_RESULT)
            call recv(node_comm, matrix_result, worker_source, TAG_WORKER_MATRIX_RESULT, status)

            ! Forward results to global coordinator with fragment index
            call send(world_comm, worker_fragment_map(worker_source), 0, TAG_NODE_SCALAR_RESULT)  ! fragment_idx
            call send(world_comm, scalar_result, 0, TAG_NODE_SCALAR_RESULT)                       ! scalar result
            call send(world_comm, matrix_result, 0, TAG_NODE_MATRIX_RESULT)                       ! matrix result

            ! Clear the mapping and deallocate
            worker_fragment_map(worker_source) = 0
            if (allocated(matrix_result)) deallocate (matrix_result)
         end if

         ! PRIORITY 2: Check for work requests from local workers
         call iprobe(node_comm, MPI_ANY_SOURCE, TAG_WORKER_REQUEST, local_message_pending, status)

         if (local_message_pending) then
            ! Only process work request if this worker doesn't have pending results
            if (worker_fragment_map(status%MPI_SOURCE) == 0) then
               call recv(node_comm, local_dummy, status%MPI_SOURCE, TAG_WORKER_REQUEST)

               if (more_fragments) then
                  call send(world_comm, dummy_msg, 0, TAG_NODE_REQUEST)
                  call recv(world_comm, fragment_idx, 0, MPI_ANY_TAG, global_status)

                  if (global_status%MPI_TAG == TAG_NODE_FRAGMENT) then
                     call recv(world_comm, fragment_size, 0, TAG_NODE_FRAGMENT, global_status)
                     allocate (fragment_indices(fragment_size))
                     call recv(world_comm, fragment_indices, 0, TAG_NODE_FRAGMENT, global_status)

                     call send(node_comm, fragment_idx, status%MPI_SOURCE, TAG_WORKER_FRAGMENT)
                     call send(node_comm, fragment_size, status%MPI_SOURCE, TAG_WORKER_FRAGMENT)
                     call send(node_comm, fragment_indices, status%MPI_SOURCE, TAG_WORKER_FRAGMENT)
                     call send(node_comm, matrix_size, status%MPI_SOURCE, TAG_WORKER_FRAGMENT)

                     ! Track which fragment was sent to this worker
                     worker_fragment_map(status%MPI_SOURCE) = fragment_idx

                     deallocate (fragment_indices)
                  else
                     call send(node_comm, -1, status%MPI_SOURCE, TAG_WORKER_FINISH)
                     finished_workers = finished_workers + 1
                     more_fragments = .false.
                  end if
               else
                  call send(node_comm, -1, status%MPI_SOURCE, TAG_WORKER_FINISH)
                  finished_workers = finished_workers + 1
               end if
            end if
         end if
      end do
   end subroutine node_coordinator

   subroutine node_worker(world_comm, node_comm, max_level, sys_geom, method)
      !! Node worker for processing fragments assigned by node coordinator
      class(comm_t), intent(in) :: world_comm, node_comm
      integer, intent(in) :: max_level
      type(system_geometry_t), intent(in), optional :: sys_geom
      character(len=*), intent(in) :: method

      integer(int32) :: fragment_idx, fragment_size, dummy_msg, matrix_size
      integer(int32), allocatable :: fragment_indices(:)
      real(dp) :: dot_result
      real(dp), allocatable :: C_flat(:)
      type(MPI_Status) :: status
      type(physical_fragment_t) :: phys_frag

      dummy_msg = 0

      do
         call send(node_comm, dummy_msg, 0, TAG_WORKER_REQUEST)
         call recv(node_comm, fragment_idx, 0, MPI_ANY_TAG, status)

         select case (status%MPI_TAG)
         case (TAG_WORKER_FRAGMENT)
            call recv(node_comm, fragment_size, 0, TAG_WORKER_FRAGMENT, status)
            allocate (fragment_indices(fragment_size))
            call recv(node_comm, fragment_indices, 0, TAG_WORKER_FRAGMENT, status)
            call recv(node_comm, matrix_size, 0, TAG_WORKER_FRAGMENT, status)

            ! Build physical fragment from indices if sys_geom is available
            if (present(sys_geom)) then
               call build_fragment_from_indices(sys_geom, fragment_indices, phys_frag)

               ! Process the chemistry fragment with physical geometry
               call process_chemistry_fragment(fragment_idx, fragment_indices, fragment_size, matrix_size, &
                                               dot_result, C_flat, method, phys_frag)

               call phys_frag%destroy()
            else
               ! Process without physical geometry (old behavior)
               call process_chemistry_fragment(fragment_idx, fragment_indices, fragment_size, matrix_size, &
                                               dot_result, C_flat, method)
            end if

            ! Send results back to coordinator
            call send(node_comm, dot_result, 0, TAG_WORKER_SCALAR_RESULT)
            call send(node_comm, C_flat, 0, TAG_WORKER_MATRIX_RESULT)

            deallocate (fragment_indices, C_flat)
         case (TAG_WORKER_FINISH)
            exit
         end select
      end do
   end subroutine node_worker

   subroutine unfragmented_calculation(sys_geom, method)
      !! Run unfragmented calculation on the entire system (nlevel=0)
      !! This is a simple single-process calculation without MPI distribution
      type(system_geometry_t), intent(in), optional :: sys_geom
      character(len=*), intent(in) :: method

      real(dp) :: dot_result
      real(dp), allocatable :: C_flat(:)
      integer :: total_atoms
      type(physical_fragment_t) :: full_system
      integer :: i

      if (.not. present(sys_geom)) then
         call logger%error("sys_geom required for unfragmented calculation")
         error stop "Missing geometry in unfragmented_calculation"
      end if

      total_atoms = sys_geom%total_atoms

      call logger%info("============================================")
      call logger%info("Running unfragmented calculation")
      call logger%info("  Total atoms: "//to_char(total_atoms))
      call logger%info("============================================")

      ! Build the full system as a single fragment (all monomers)
      block
         integer, allocatable :: all_monomer_indices(:)

         allocate (all_monomer_indices(sys_geom%n_monomers))
         do i = 1, sys_geom%n_monomers
            all_monomer_indices(i) = i
         end do

         call build_fragment_from_indices(sys_geom, all_monomer_indices, full_system)
         deallocate (all_monomer_indices)
      end block

      ! Process the full system with verbosity=1 for detailed output
      block
         integer, allocatable :: temp_indices(:)
         allocate (temp_indices(sys_geom%n_monomers))
         do i = 1, sys_geom%n_monomers
            temp_indices(i) = i
         end do

         call process_chemistry_fragment(0, temp_indices, sys_geom%n_monomers, &
                                         total_atoms, dot_result, C_flat, &
                                         method, phys_frag=full_system)
         deallocate (temp_indices)
      end block

      call logger%info("============================================")
      call logger%info("Unfragmented calculation completed")
      block
         character(len=256) :: result_line
         write (result_line, '(a,es15.8)') "  Scalar result: ", dot_result
         call logger%info(trim(result_line))
      end block
      call logger%info("  Matrix size: "//to_char(size(C_flat)))
      call logger%info("============================================")

      if (allocated(C_flat)) deallocate (C_flat)

   end subroutine unfragmented_calculation

end module mqc_mbe
